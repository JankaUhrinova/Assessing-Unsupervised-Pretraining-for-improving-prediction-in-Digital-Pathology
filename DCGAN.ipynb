{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvbBUrhmNymR936j0WFYU6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JankaUhrinova/Assessing-Unsupervised-Pretraining-for-improving-prediction-in-Digital-Pathology/blob/main/DCGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCH4UIM5Hf-B",
        "outputId": "6653a15d-ee51-41e9-ee65-fb8cc1694cae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Seed:  9457\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f6b0b829cf0>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "#%matplotlib inline\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voAeXyz0IIGE",
        "outputId": "97c496f3-df14-4add-cfe1-5e94db26bf7a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataroot = \"data/MNIST\"\n",
        "\n",
        "# Number of workers for dataloader\n",
        "workers = 1\n",
        "\n",
        "# Batch size during training\n",
        "batch_size = 32\n",
        "\n",
        "# Spatial size of training images. All images will be resized to this\n",
        "#   size using a transformer.\n",
        "image_size = 28\n",
        "\n",
        "# Number of channels in the training images. For color images this is 3\n",
        "nc = 1\n",
        "\n",
        "# Size of z latent vector (i.e. size of generator input)\n",
        "nz = 100\n",
        "\n",
        "# Size of feature maps in generator\n",
        "ngf = 32\n",
        "\n",
        "# Size of feature maps in discriminator\n",
        "ndf = 32\n",
        "\n",
        "# Number of training epochs\n",
        "num_epochs = 2\n",
        "\n",
        "# Learning rate for optimizers\n",
        "lr = 1e-1\n",
        "\n",
        "# Beta1 hyperparam for Adam optimizers\n",
        "beta1 = 0.5\n",
        "\n",
        "# Number of GPUs available. Use 0 for CPU mode.\n",
        "ngpu = 1"
      ],
      "metadata": {
        "id": "62X2_Gw8V8JU"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.MNIST(root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.Compose([\n",
        "                               transforms.Resize(image_size),\n",
        "                               transforms.CenterCrop(image_size),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5), (0.5)),\n",
        "                               ])\n",
        ")"
      ],
      "metadata": {
        "id": "W1nU3QgsJLoL"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=workers)\n"
      ],
      "metadata": {
        "id": "FM2QZLQWTc7I"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_map = {\n",
        "    0: \"0\",\n",
        "    1: \"1\",\n",
        "    2: \"2\",\n",
        "    3: \"3\",\n",
        "    4: \"4\",\n",
        "    5: \"5\",\n",
        "    6: \"6\",\n",
        "    7: \"7\",\n",
        "    8: \"8\",\n",
        "    9: \"9\",\n",
        "}\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(dataset), size=(1,)).item()\n",
        "    img, label = dataset[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(labels_map[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "zUwBKJHIUw7I",
        "outputId": "0d2dc247-d1c3-4bdc-a6e5-34709c6d48e0"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 9 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAckAAAHRCAYAAAABukKHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de7zVU/748ffihKaLnFy6qYOkGZcply5KuY2kVNTJpUFSyLWURvQb1CghIsIkjFxDmnpMchnlkgqZwUhS6S5RoaKmTp/vH40fa73XOnuf3d77s8/Zr+fj0R/v93nvz2fN+LTffc5an/UxURQJAADQdot7AAAA5CqaJAAAATRJAAACaJIAAATQJAEACKBJAgAQQJMEACCAJukwxmxy/pQYY8bEPS5UfMaYJ40xXxljfjDGLDTG9I57TKjYjDEzjTFbfvV993ncY8o1NElHFEVVf/4jIrVE5CcReT7mYSE/jBCRoiiKqotIJxH5izHmmJjHhIrvql997x0W92ByDU2ydF1FZK2IvB33QFDxRVH0aRRFW38O//fnkBiHBOQ9mmTpLhKRJyL27kOWGGPGGmN+FJEFIvKViEyLeUio+EYYY741xswyxpwY92ByjeH7388Y00BElohIwyiKvox7PMgfxpjdRaSliJwoIiOjKNoW74hQURljmovIfBH5r4icKyL3i0iTKIoWxzqwHMKdZNgFIvIODRLZFkVRSRRF74hIPRHpG/d4UHFFUTQ3iqKNURRtjaLobyIyS0TOiHtcuYQmGXahiPwt7kEgrxUIc5LIrkhETNyDyCU0SQ9jzPEiUldY1YosMcbsb4w51xhT1RizuzGmnYicJyL/jHtsqJiMMTWMMe2MMXsZYwqMMT1EpI2ITI97bLmkIO4B5KiLRGRSFEUb4x4I8kYkO3+1+pDs/MfrMhHpF0XRlFhHhYqskoj8RUQai0iJ7Fws1iWKooWxjirHsHAHAIAAft0KAEAATRIAgACaJAAAATRJAAACaJIAAASU+giIMYalr3ksiqJYHirmustvcVx3XHP5rbRrjjtJAAACaJIAAATQJAEACKBJAgAQQJMEACCAJgkAQABNEgCAAJokAAABNEkAAAJokgAABNAkAQAIoEkCABBAkwQAIIAmCQBAAE0SAIAAmiQAAAE0SQAAAgriHgBQHjRs2NCKO3furGruuOOOMh93t930v1N37NihcpMnT7biqVOnqpp33nnHihctWlTm8QC7olu3bip3/PHHW3H//v1VTd++fVXuoYceSt/AdgF3kgAABNAkAQAIoEkCABBAkwQAIICFO0ASWrdubcW33367qvEtuEmF7zidOnUqNRYRWbBggRWfeeaZqmbNmjVWvGXLllSGiDxUu3ZtlZs+fboVH3744arGXZwWRVF6B5Zh3EkCABBAkwQAIIAmCQBAQGxzkhMnTrTiFi1aqJo5c+ao3IoVK6x47ty5Cc+1fPnypI4NlGeNGze24sWLF6ua0aNHW/GAAQMyOiaUD927d1e5m266yYrdDTVERCpXrpzw2K+88ooVv//++6pm/PjxCY8TF+4kAQAIoEkCABBAkwQAIIAmCQBAgCntwU5jTMae+jzwwAOtuGXLlqqmefPmCT/nU1xcXObxzJ49W+VWrlypci+88IIVuwuQKpIoikwc583kdZeqqlWrWvFhhx2maty3HUyZMkXV9OrVy4oXLlyoai6++GKVKyoqsuIGDRoEx/oz3xtGVq9ebcUnnHCCqlm6dGnCY2dSHNddLl5z6eJeO9dcc42queqqq1SuoCDxus6XX37ZikeOHKlq3MWVW7duTXjcbCvtmuNOEgCAAJokAAABNEkAAAJokgAABMS2cCebfLv51K9f34q7deumaurVq6dy7gIj34Kfc845x4rdXYLKCxbu5I4mTZpYsW9x2qBBg6zYt3DHfcPIsGHDVM3QoUNTGWLasHAndb7vMfe/p7sz067o0qWLFfsWq5UHLNwBACAFNEkAAAJokgAABOTFnGQ6uZsZ+N4w4s5JltcNB5iTzF0nnniiyv3zn/+04mTmJH0OPvhglVu2bFnyg9tFzEn6+d7Ccdlll1nxtddeq2rcTQFWrVqlambOnKly7qYSvjd1rF271op//PFHVVMeMCcJAEAKaJIAAATQJAEACKBJAgAQkHibd5SZb4MBYFfUqFHDim+88UZVk8yinGRqkJvatm2rcgMGDEj4OXdBl+8tM743HrkKCwtVrmPHjlZc2kLQnxmj18jMmzdP5RYvXpzwWNnAnSQAAAE0SQAAAmiSAAAEMCdZRv369UtYU143NEdu6Nmzp8pdcMEFVtymTZssjQZxad++vRUnM/943333qdw999xjxb75x9atW6tc06ZNrfiaa65RNYccckjCMSVj0aJFKnf66adb8ZIlS9JyrrLiThIAgACaJAAAATRJAAACaJIAAASwcKcU7hs/RESuu+46K7777ruzNRxUAO5b4WvVqqVqrr76apU76qij0nL+LVu2WLFvUwLfWyKQWUVFRSr37LPPWnG1atVUzcsvv2zFo0ePVjVnn322FZ9//vmqxr0uRUSqVKniHWsm+N5w8o9//MOKf/vb32ZrOBbuJAEACKBJAgAQQJMEACCAOclStGzZMmHN3LlzszASlEd33nmnynXq1MmKfXMxmdyEfPjw4VY8ZsyYjJ0Lfg0aNFC56dOnq5xvDtL11ltvWfGECRNUTatWrcowul/861//suJPP/1U1Tz00ENlPu7gwYNVrkOHDipXp06dMh87E7iTBAAggCYJAEAATRIAgACaJAAAASzc2UWzZ8+OewiIQdWqVVXuiSeesOLOnTsnPM5uu2Xu36mXXXaZyj3yyCMZOx+SM2jQIJVr1KhRSscaMWJEwpr169db8WeffaZqxo0bp3JTp0614g0bNpRxdH5r165Nqs79u1G3bl1Vk42NL7iTBAAggCYJAEAATRIAgACaJAAAASzcKYVvoti1YsWKlI7dvXt3K27evLmq8b2FxH3ryJw5c1I6P3ZNjRo1VK5NmzZWnOrOOal+bvz48VbMIp3c1Lt374wd+z//+Y/KuQuFfLv75CJ34U6lSpXiGUcsZwUAoBygSQIAEECTBAAgoNzNSbZo0cKK69evn7Fz9e/fP2HN8uXLVc43l+h6/vnnrdi3KYHvLeOpzoEivVauXKly7hs1hgwZkq3hiIjIJZdcYsW+uafJkydnazgIeOqpp1SuZ8+eKrdu3Tordh/uF9H/Pd944w1Vs2nTpjKOML322GMPKy4sLEzqc+5GAUuXLk3XkMqEO0kAAAJokgAABNAkAQAIoEkCABBQ7hbutGzZ0op9i2uSWTiTLr6H+QcOHGjFEydOzNZwEKN7773Xin/3u9+pmuOPP96KFy5cqGqiKFK5oqIiK27QoEHC8Vx66aUqN3PmTCv+7rvvEh4H6TVy5EiVmzZtmsotWbLEij/88MOMjSld9txzT5W75ZZbrNj3dpySkhKVGzZsWNrGtSu4kwQAIIAmCQBAAE0SAIAA45v/+P8/NCb8wzzgm0ssLi62YmNMtoaTdVEUxfI/riJfd61atbLiWbNmJfW5Jk2aWLFvo4CaNWtasbtBtIjIk08+acUXXXRRUufPpjiuu4p8zWWTu9mLiMi7776b8HOLFi1SuUaNGqVlTMko7ZrjThIAgACaJAAAATRJAAACaJIAAASUu80EgPLMXejle/uDT6dOnax4v/32S/gZ38Kd1q1bJ3U+xK927dpWvGXLFlWzYcOGbA1Hvc1DRKRfv35WfPnllyc8ju9NRu71nUu4kwQAIIAmCQBAAE0SAIAA5iTLaPbs2XEPAUm65557VO73v/+9FS9YsEDV3HHHHSrnzuX16tUrpTEdeuihVlynTh1Vs2PHjoTHSabGp7TNQxCfk046SeXcTc+//PJLVePO5flqCgrsr/lq1aolNSb3Gm/WrJmqOfvssxMex71Wb7jhBlXj+3uYK7iTBAAggCYJAEAATRIAgACaJAAAASzcKaN69erFPQQkyV2kIyJywgknlBqLiPTp00fl3AfzU104A/g0btxY5dyH9301CxcutOIJEyaomn322ceKO3bsmMoQvdy/B8uWLVM1w4YNs+JnnnkmbefPBu4kAQAIoEkCABBAkwQAIIAmCQBAgCltBw5jTF5vz5HM7iTuWx0qkiiKYvkfl67rbuzYsSrnW5STjEwt3PG9qSOZY8+ZM0fltm7dasWXXHKJqtm+fbsVr1q1KuG5si2O6y4Xv+tWr15txbVq1crYuXzfddu2bbNi37XiLsp5/PHH0zqubCntmuNOEgCAAJokAAABNEkAAAKYkyzFxIkTVa64uNiKmZNMv3Rdd1WrVlU5960Fvjd+1KxZU+WyOSc5fPhwlfviiy+seNKkSapm06ZNaRlT3JiT9Lv22mtVzr3GTz311ITHef3111XOtwnAk08+WYbRlW/MSQIAkAKaJAAAATRJAAACaJIAAASwcKcULNwp3wt3UD6xcAfZxsIdAABSQJMEACCAJgkAQEBB3AMob1asWBH3EAAAWcKdJAAAATRJAAACaJIAAATQJAEACGAzAQSxmQDiwGYCyDY2EwAAIAU0SQAAAmiSAAAE0CQBAAigSQIAEECTBAAggCYJAEAATRIAgIBSNxMAACCfcScJAEAATRIAgACaJAAAATRJAAACaJIAAATQJAEACKBJAgAQQJMEACCAJgkAQABNEgCAAJqkhzGmyBgzzRizwRizxhhzvzGmIO5xoeIyxmxy/pQYY8bEPS5UbMaYQmPMS8aYzcaYZcaY8+MeU66hSfqNFZG1IlJbRJqISFsRuSLWEaFCi6Ko6s9/RKSWiPwkIs/HPCxUfA+IyH9F5AAR6SEiDxpjDo93SLmFJul3kIhMjKJoSxRFa0Rkuohw4SBbusrOf6S9HfdAUHEZY6rIzmvt/0VRtCmKondEZIqIXBDvyHILTdJvtIica4z5jTGmroi0l52NEsiGi0TkiYhX9CCzGonI9iiKFv4q95FwQ2ChSfq9JTsvlB9EZKWIfCAik2MdEfKCMaaB7Pz1/t/iHgsqvKqy8zvu174XkWoxjCVn0SQdxpjdZOdd4yQRqSIi+4rIPiIyMs5xIW9cICLvRFH0ZdwDQYW3SUSqO7nqIrIxhrHkLJqkVigi9UXk/iiKtkZRtE5EHhORM+IdFvLEhcJdJLJjoYgUGGMO/VXu9yLyaUzjyUk0SUcURd+KyJci0tcYU2CMqSE754g+jndkqOiMMceLSF1hVSuyIIqizbLzN2ZDjTFVjDGtRKSziEyId2S5hSbpd7aInC4i34jIIhHZJiL9Yx0R8sFFIjIpiiJ+3YVsuUJEKsvO1dTPiEjfKIq4k/wVwwI6AAD8uJMEACCAJgkAQABNEgCAAJokAAABpb7ZwhjDqp48FkWRieO8XHf5LY7rjmsuv5V2zXEnCQBAAE0SAIAAmiQAAAE0SQAAAmiSAAAE0CQBAAigSQIAEECTBAAggCYJAEAATRIAgACaJAAAATRJAAACaJIAAATQJAEACKBJAgAQQJMEACCAJgkAQABNEgCAAJokAAABNEkAAAJokgAABNAkAQAIoEkCABBAkwQAIIAmCQBAAE0SAICAgrgHUN4ce+yxVnzGGWck/Mzy5ctVbsKECVZcUlKyawP7lSOOOMKKn3766YSfOeqoo9J2fgC7rl69elY8bNgwVbNhwwYrvu666zI6JtfBBx9sxYcffriqmTp1araGkxHcSQIAEECTBAAggCYJAEAATRIAgAAW7pTihhtuUDl3YnzfffdN6dgnnniiFffs2TOpz1WvXt2KBwwYoGr69OljxYsWLVI1l19+eVLnQ3q1aNHCit3/niH77LOPFSezYMwYo3Jdu3a14vnz56ua008/XeXWrVuX8HxITo0aNVRu9OjRKnfWWWdZcbVq1VTNt99+a8WTJ09OeP42bdqonO97JBm77767FRcU6JaydOlSK/Z992zfvl3l3n333ZTGlG7cSQIAEECTBAAggCYJAEAATRIAgAATRVH4h8aEf1jBfP/99ypXtWpVlXvllVeseNy4cQlrXnzxRVVTqVIlK27Xrp2qad68ucq99NJLVlyzZk1V079/fyt+4IEHVM2OHTtUzhVFkV75kQXl9bpz/3sNHz5c1bRu3dqKfQsdfAtuSvt7GpLqcXwLd1577bUynz9VcVx3mbzmzjvvPCvu0qWLqikuLs7U6csFd+cgEf93W6aUds1xJwkAQABNEgCAAJokAAABebuZgDuXWKVKFVXjm8u78cYbrXjTpk0JzzV48GCVW716tRW7c40iIh07dlS5efPmWXGrVq1UjW/zAKRX5cqVVW7atGlW7HtoPFXbtm2zYt98o29+E9nl2xyie/fuVty5c+dsDcfLt/5ixYoVKuduXtCgQYOMjcndlEBEvwVl5cqVGTt/abiTBAAggCYJAEAATRIAgACaJAAAAXkx0/+b3/xG5Zo1a2bFvrccjBw5UuWSWajj2muvvVTuhRdesGL37RAiIldccYXKPfbYY1a8devWMo8Hu27s2LEq576pI5kH95csWaJyvs0nknm7w5gxY6z4mGOOSfiZ559/XuU+/PBDlXM3RnAXsGGnk08+WeWyuVDHXdgnIjJ9+nQr/uSTT1TNxIkTVa5hw4ZW/Oyzz6qao48+uqxD9PIteHKv+fPPP1/VLFy4MC3nLw13kgAABNAkAQAIoEkCABCQFxucu5tKi4jMmDHDin2/23bnLUVENm/ebMW+TQieeOIJK+7QoYOq+eabb6z4wgsvTDjGbGOD81/UqlXLit3NIHw2btyocgMHDrRi3wb5yWjSpInK+eYSE42padOmqsY3T+pu2u/bkD9dyvMG54sXL1a5gw46KKVjvf/++1Z85ZVXJvzM2rVrVW758uUpnd/l20xgv/32s+IePXqoGneD9/333z+l8/v+vz300ENTOpaLDc4BAEgBTRIAgACaJAAAATRJAAAC8mIzAR9313nfGxvcXfB9n3v77bdVzZFHHmnFn376qao599xzE9Ygd/kWvLlv5rjhhhtUTaoLderUqWPFvs0F3DH53hTiXne+RTo+ydblu88//1zlUl244y54ifu/wbJlyxLmPvjgA1XjbkQxadIkVeN7m5HrkEMOSViTCdxJAgAQQJMEACCAJgkAQEBezEn65vu+/vprK3Y3CRARufTSS1Wub9++VnzAAQeomn79+lmxbxPpr776yj9Y5CT3+vDNq7z77rtWnOr8o0/btm2t+MADD0z4mQEDBqjca6+9ltL53esefj179lS5CRMmWPEf/vCHLI0mN7gbp/z9739XNcnMScaFO0kAAAJokgAABNAkAQAIoEkCABCQFwt3tm7dqnILFiywYndhhIjILbfconLfffedFbsPeYvoieqSkpJkhokc5r49o7i4OGPnOuaYY1TutttuS/i5v/71r1Z8zz33pG1MSI7vLRzJvDEGuYs7SQAAAmiSAAAE0CQBAAigSQIAEJAXC3e2bNmicr4J9mRqWrRoYcVr1qxJfWCAx5lnnqlyRUVFVvz999+rmqFDh2ZqSEDe4k4SAIAAmiQAAAE0SQAAAirknGRhYaEVd+zYUdUk8zD4ypUrVW7p0qUpjwtw+TYOuP7661UuiiIrnjp1qqrhzTK5yX2bUJcuXVTN3nvvrXLz58+34ttvv13V+DY8QXpxJwkAQABNEgCAAJokAAABNEkAAALK/cId38KHBx980IqPPfZYVTNz5kwr3rx5s6pp3769yrVr186KX3nllWSGCYiIyO67727F5513nqqpXLmyyrkbW/zpT39K78CQMdu2bUvpc3vssYcV+xYgPvbYY1a8bNmylM6FMO4kAQAIoEkCABBAkwQAIKDczUm684u+t683bdrUin3zN+5b3Bs2bKhqfHOS3bp1s+JXX31V1bgPfgM/O/fcc624f//+SX1u1qxZVszGAeXXxRdfrHKTJk1K+Lmjjz5a5dq2bWvFb775pqqpKPOUvXr1iuW83EkCABBAkwQAIIAmCQBAAE0SAICAnF64477NQ0Tk6quvtuJWrVqpGncxz5133pnwXPPmzVM535sWLrnkEit2Ny4QEfnwww8Tng8VX7Vq1VRu6NChVmyMUTXr169Xud69e6dvYClwN81wN9VA8nzfD+7CLBH/d5vr8ccfT3hsd7HYokWLEh43F/n+XmQDd5IAAATQJAEACKBJAgAQkNNzkvfee6/K9ejRw4offfRRVTNw4MC0nN+3wYDLt8E6c5IQEbn++utVrqioyIp9G0+89tprKrdhw4a0jSsVbJCRPsuXL1e54uJilZsyZYoV+17U4PJtOPD6669b8caNGxMeR0Tk3//+txXfeuutqubQQw+14iZNmqiaE044wYrdzV5CXnrpJSueMWNGUp9LN+4kAQAIoEkCABBAkwQAIIAmCQBAQE4v3PnjH/+ocu7Ds77FEaksMqhUqVKZPyMismLFipQ+h4qncePGVnzhhRcm/MyPP/6ocnfddVfaxoTyYc2aNSrXuXNnK77uuutUjfsdecABB6ia+vXrpzSmww8/3IrdRZOp8m2gsWXLFpWbOHGiFSe74CjduJMEACCAJgkAQABNEgCAAJokAAABOb1wZ/Xq1Sp30UUXWXG9evVUjW8SPJFzzjlH5QoK9P897i4Ub731VpnPhYpp8ODBVnzggQcm/MyVV16pcr430iD/fPXVV1bsW6Q4ffp0K3YXu4jot9Hs2LEjqfPvscceCWtKSkqseNu2barmv//9rxW/+OKLqmbcuHEqN2fOnITnzwbuJAEACKBJAgAQQJMEACDAlPbgvTEm1q3/CwsLVW7UqFFW7O4wn6rZs2er3MiRI1Vu8eLFVvzTTz+l5fy5KIoi/dRvFsR93SWjTp06Krdq1Sor9v3dWrBggRWfcsopqsadi8oFw4cPt+Ibb7wxY+eK47orD9dcqgYMGGDFmzZtUjW+B/yPO+64hMdeunSpFc+fP1/VuHOL7t+TXFDaNcedJAAAATRJAAACaJIAAATQJAEACMjphTuIFwt3wiZPnqxynTp1smLf360hQ4ZY8YgRI9I7sAqAhTvINhbuAACQApokAAABNEkAAAJyeoNzIFfttddeCWs++ugjlRs9enQmhgMgQ7iTBAAggCYJAEAATRIAgACaJAAAASzcATJk2LBhKleR3xoDVETcSQIAEECTBAAggCYJAEAAG5wjiA3OEQc2OEe2scE5AAApoEkCABBAkwQAIIAmCQBAQKkLdwAAyGfcSQIAEECTBAAggCYJAEAATRIAgACaJAAAATRJAAACaJIAAATQJAEACKBJAgAQQJMEACCAJukwxuxpjBlvjFlmjNlojPm3MaZ93ONCxWeMKTTGvGSM2fy/6+/8uMeEiovvuuTQJLUCEVkhIm1FZG8RGSIiE40xRTGOCfnhARH5r4gcICI9RORBY8zh8Q4JFRjfdUlgg/MkGGM+FpFboyh6Me6xoGIyxlQRkQ0ickQURQv/l5sgIquiKLoh1sEhb/Bdp3EnmYAx5gARaSQin8Y9FlRojURk+88N8n8+EhHuJJEVfNf50SRLYYypJCJPicjfoihaEPd4UKFVFZEfnNz3IlIthrEgz/BdF0aTDDDG7CYiE2TnHNFVMQ8HFd8mEanu5KqLyMYYxoI8wndd6WiSHsYYIyLjZecCiq5RFG2LeUio+BaKSIEx5tBf5X4v/OoLGcR3XWIs3PEwxjwkIk1E5NQoijbFPR7kB2PMsyISiUhv2Xn9TROR46MoolEiI/iuS4wm6TDGNBCRpSKyVUS2/+pHl0VR9FQsg0JeMMYUisijIvIHEVknIjdEUfR0vKNCRcV3XXJokgAABDAnCQBAAE0SAIAAmiQAAAE0SQAAAmiSAAAEFJT2Q2MMS1/zWBRFJo7zct3ltziuO665/FbaNcedJAAAATRJAAACaJIAAATQJAEACKBJAgAQQJMEACCAJgkAQABNEgCAAJokAAABNEkAAAJokgAABNAkAQAIoEkCABBQ6ltAAABIVkGBbinVq1e34m3btqmajRs3ZmxMu4o7SQAAAmiSAAAE0CQBAAgo93OSvt+Bd+7c2YpPPvlkVXPFFVckPPbKlStV7vHHH7fi8ePHq5qlS5cmPDYQcuyxx6rcZZddZsW1atVSNe3atbNi9++BiMjLL7+8i6NDeVelShWVq127tsodd9xxVnzSSSclPHbNmjVV7qyzzrJi3/fj6aefrnILFy5MeL5s4E4SAIAAmiQAAAE0SQAAAmiSAAAEmCiKwj80JvzDmLiTzm+//baqadKkSbaGI7fddpvK3XLLLVZcUlKSpdGkVxRFJo7z5uJ1lyldu3ZVuYcffljlCgsLy3zsTZs2qdzZZ59txa+//nqZj5tpcVx3+XTNPfrooyrXs2fP7A/kV3yLeW6++WYrnjBhQsbOX9o1x50kAAABNEkAAAJokgAABOT0nKTvodfu3btbse9hfteOHTtU7rvvvkv4uWQ26/Vxf5c+bNiwhJ/JRcxJpp+7UcC0adNUzb777qtyq1atsuJHHnlE1bgPXxcXFyc8dps2bcKDjQlzkpk1atQolWvQoIHKvffee1Y8b948VeP7bk1k7NixKnfYYYep3PLly624qKiozOdKFnOSAACkgCYJAEAATRIAgACaJAAAATnzFhBj9LzpHXfcoXJ9+/ZNeCx3wnfw4MGq5plnnkl4HN+bFqZOnWrFxxxzjKpxF0NUqlRJ1fjezo2Kb8SIEVa83377qRrfYjT3Gv7mm28Snsu30GL27NlW3L59e1XDm0IqtgEDBqjc/vvvr3Jr165Ny/maNWtmxfXr10/qc/fff39azr+ruJMEACCAJgkAQABNEgCAAJokAAABObNwx7cLfSqLdERE2rVrZ8Wff/55SmNas2aNyg0cONCKZ8yYoWpOOeUUK27VqpWqmTlzZkpjQvlx3nnnqVzbtm2t+I033lA1l156qcqlsrOJuwOPiH4zSJ8+fVQNC3fyT7oW6fh8/PHHVvzFF1+omqOOOkrl9txzz4yNqSy4kwQAIIAmCQBAAE0SAICAnJmT9L0t24XOmIwAAAhvSURBVPeGEncO8rTTTlM1vrmYOF199dUqx5xkxeebZ1+5cqUVX3HFFaomlfnHVD399NNZOxfyU9euXa24Zs2aquaDDz5QuTFjxmRsTGXBnSQAAAE0SQAAAmiSAAAE0CQBAAjImYU7vodZfW/qGDt2rBVne5GO760NgIh+k4LvDTHuW0BS3egiXX766adYz4+Kr1GjRlZct25dVTNs2DCV++GHHzI2prLgThIAgACaJAAAATRJAAACcmZO0t34WURkwYIFMYzkF4cddpjKPfDAA2U+zvPPP5+O4SDHDRo0yIoLCwtVzdy5c7M1HKlatarKVapUKWvnR/45+uijVa53795W/OWXX6qaFStWZGxMu4o7SQAAAmiSAAAE0CQBAAigSQIAEJAzC3fiXqTjc+SRR6pcMpsJvP/++1bMm97zw/nnn2/FM2bMUDVz5szJ1nDUxgUieuHO7NmzszUc5IGBAweqXO3ata24WbNmqsb3FpBcwZ0kAAABNEkAAAJokgAABOTMnGTcbr31VpXzvTU+Ge7v17///vuUjoPcdcQRR6jc3nvvbcVTpkxRNdu3b8/YmFwHHHCAyrnzpOvXr8/WcFCKevXqqZz7YH7//v1VTUFB4q/wSZMmWfGsWbNUzXvvvZfwOD6tW7e24lNPPVXVrFu3zoq/++67lM4VF+4kAQAIoEkCABBAkwQAIIAmCQBAQN4u3DnllFOs2LdIp2bNmgmP43sIdvDgwakPDOVCv379VK5y5cpW/Nlnn2VrOCKiFw75FlGMGTMmW8PB/7gbkHTo0EHVXHPNNSrXpEmTtJzfXVyzceNGVePb8GTUqFFW3LhxY1Vz4YUXWvH8+fNVzc0332zFixYtCg82B3EnCQBAAE0SAIAAmiQAAAE0SQAAAvJi4c5JJ52kcs8995wVFxYWJnUsd4eSIUOGqJoffvihDKNDeeC+PaNRo0aqZuvWrVac7d1s3GuxRo0aquatt97K1nDyUlFRkcrNmzfPivfZZ5+kjvXRRx9Z8RtvvJHSmOrWrWvF3bt3VzXFxcUqd8YZZ1hxlSpVEp6rR48eKvfmm28m/Fwu404SAIAAmiQAAAE0SQAAAsr9nKTvgdumTZta8V133aVqkpkX+OSTT1Sua9euVlzeHoxFatz5PfcBbRGR5cuXW3Em37bue/tD586drfihhx5SNcxJppe7UcALL7ygatxNHjZv3qxqrr/+epUbN26cFZeUlKQyRHWtrFmzRtX4NjNIZg7S3ZjAt5lAecedJAAAATRJAAACaJIAAATQJAEACCh3C3datWplxVOmTFE1yT6sm4hvgp2FOghZtWpV1s7VrVs3lWvYsKEV33TTTapm27ZtGRtTPlq8eLEVV61aVdW4mwK0bNlS1WzZsiUt43E3vRARue2226zYt0gnVdWrV7figQMHqhr3Ovz6669VjbsRRy7hThIAgACaJAAAATRJAAACTBRF4R8aE/5hFgwaNEjlBg8ebMXug7rp5JsnWLlypRU/8cQTqubhhx+24m+++Sa9A8uSKIpMHOeN+7rzcR8a982ruG9gHzZsWNrO717n7nUoojfSdjeoFhH58ccf0zamTInjukv1mhswYIAV33nnnarGXdvge+FCMhtP1KpVS+XcDcVPPvlkVdO+fXsr9q2ruO+++1Ru3bp1Vuyb7xw1apQV16xZU9W4mjVrpnKZ3HgjGaVdc9xJAgAQQJMEACCAJgkAQABNEgCAgJzZTOCpp55SuQ4dOqic+/BqJu21114q5z6wPXToUFVz2mmnWbFvcY/7EPKMGTNSGSLyxI033mjFvjc0uA+Nl4dFOuVdMm+9cBdZVatWTdVcddVVKtexY0crPuSQQ1SNm/vss89UzZVXXmnFzz77rKrZsGGDyiWjdu3aVjxixAhV474dx/cWklzGnSQAAAE0SQAAAmiSAAAE5MxmAr5xlDa20rjze7NmzVI1X331lRX7NuY96KCDUjp/MtyNCt5++21VM3ny5ITHadGihcq5c6K+Y3fv3j3hsdlM4BfJbCZw1113WbFvMwyXb469T58+Knf77bdbsW8uzH2Q3H0YvLwoT5sJFBTYyzr+8pe/qJpkroNk+L4PJ0yYYMW+77Fvv/02Lef32XPPPa341FNPVTXjx4+3YneuVYTNBAAAKJdokgAABNAkAQAIoEkCABCQMwt3iouLVe65555TOXfBi7tYQkQ/VJ3MW699bxQvKipSuV69elmxb9x169ZNeL647bZb4n8fsXDnF8ks3HEXyhx11FGqxl0w5tuMYsiQISr3+eefW7HvTRLl7SHtkPK0cMfle1NG8+bNrfiCCy5I6lgPPvigFf/www+qZsmSJWUYXTzq1KljxevXr1c1vjcuZRMLdwAASAFNEgCAAJokAAABNEkAAAJyZuGOMXre1PemA3e8mzdvztiYklG5cmWV6927txX7Fll06dIlY2Nyde7cWeWmTp2a8HMs3PmFe326b1YQEbn77ruteMeOHaqmpKTEit0dS0REVq9erXLu4g93AVBFUp4X7qB8YuEOAAApoEkCABBAkwQAICBn5iQrMt+D+7vvvrsV+x48980luubOnatyr776qhVv375d1STzhhXmJMvGfXvHn//8Z1XjvqX+k08+UTX333+/yn388ce7OLrygzlJZBtzkgAApIAmCQBAAE0SAIAAmiQAAAEs3EEQC3cQBxbuINtYuAMAQApokgAABNAkAQAIoEkCABBAkwQAIIAmCQBAAE0SAIAAmiQAAAE0SQAAAmiSAAAE0CQBAAigSQIAEECTBAAggCYJAEAATRIAgACaJAAAATRJAAACTBTxQm4AAHy4kwQAIIAmCQBAAE0SAIAAmiQAAAE0SQAAAmiSAAAE/B8sG7QWqJ+yEQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "metadata": {
        "id": "b2joks7aX4mz"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator Code\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d( nz, ngf * 8, 4, 2, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 3, 2, 2, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d( ngf * 2, nc, 3, 2, 1, output_padding = 1, bias=False),\n",
        "            # state size. (ngf) x 28 x 28\n",
        "            nn.Tanh()\n",
        "            \n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "metadata": {
        "id": "yqkrIQZcyAaz"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the generator\n",
        "netG = Generator(ngpu).to(device)\n",
        "\n",
        "# Handle multi-gpu if desired\n",
        "if (device == 'cuda') and (ngpu > 1):\n",
        "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
        "\n",
        "# Apply the weights_init function to randomly initialize all weights\n",
        "#  to mean=0, stdev=0.02.\n",
        "netG.apply(weights_init)\n",
        "\n",
        "# Print the model\n",
        "print(netG)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRoaLkSC2-Fl",
        "outputId": "3bf8f154-8ce1-4cf8-c051-80926a7c93ed"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator(\n",
            "  (main): Sequential(\n",
            "    (0): ConvTranspose2d(100, 256, kernel_size=(4, 4), stride=(2, 2), bias=False)\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): ConvTranspose2d(64, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
            "    (10): Tanh()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(nc, ndf, 3, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(ndf, ndf * 2, 3, 2, 2, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "metadata": {
        "id": "kBjUqMp33IIr"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Discriminator\n",
        "netD = Discriminator(ngpu).to(device)\n",
        "\n",
        "# Handle multi-gpu if desired\n",
        "if (device == 'cuda') and (ngpu > 1):\n",
        "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
        "\n",
        "# Apply the weights_init function to randomly initialize all weights\n",
        "#  to mean=0, stdev=0.2.\n",
        "netD.apply(weights_init)\n",
        "\n",
        "# Print the model\n",
        "print(netD)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xUtqbi44ogW",
        "outputId": "b7125d8e-f8a7-4c9a-9dbd-4db0e07dba79"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discriminator(\n",
            "  (main): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (5): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (8): Conv2d(128, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (9): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize BCELoss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Create batch of latent vectors that we will use to visualize\n",
        "#  the progression of the generator\n",
        "fixed_noise = torch.randn(28, nz, 1, 1, device=device)\n",
        "\n",
        "# Establish convention for real and fake labels during training\n",
        "real_label = 1.\n",
        "fake_label = 0.\n",
        "\n",
        "# Setup Adam optimizers for both G and D\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
      ],
      "metadata": {
        "id": "hDsN8oC14uXb"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "\n",
        "# Lists to keep track of progress\n",
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "iters = 0\n",
        "\n",
        "print(\"Starting Training Loop...\")\n",
        "# For each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    # For each batch in the dataloader\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        ## Train with all-real batch\n",
        "        netD.zero_grad()\n",
        "        # Format batch\n",
        "        real_cpu = data[0].to(device)\n",
        "        b_size = real_cpu.size(0)\n",
        "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
        "        # Forward pass real batch through D\n",
        "        output = netD(real_cpu).view(-1)\n",
        "        #print(output.size(), label.size())\n",
        "        # Calculate loss on all-real batch\n",
        "        errD_real = criterion(output, label)\n",
        "        # Calculate gradients for D in backward pass\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        ## Train with all-fake batch\n",
        "        # Generate batch of latent vectors\n",
        "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
        "        # Generate fake image batch with G\n",
        "        fake = netG(noise)\n",
        "        #print(fake.size())\n",
        "        label.fill_(fake_label)\n",
        "        # Classify all fake batch with D\n",
        "        output = netD(fake.detach()).view(-1)\n",
        "        # Calculate D's loss on the all-fake batch\n",
        "        errD_fake = criterion(output, label)\n",
        "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        # Compute error of D as sum over the fake and the real batches\n",
        "        errD = errD_real + errD_fake\n",
        "        # Update D\n",
        "        optimizerD.step()\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label)  # fake labels are real for generator cost\n",
        "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "        output = netD(fake).view(-1)\n",
        "        # Calculate G's loss based on this output\n",
        "        errG = criterion(output, label)\n",
        "        # Calculate gradients for G\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        # Update G\n",
        "        optimizerG.step()\n",
        "\n",
        "        # Output training stats\n",
        "        if i % 50 == 0:\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                  % (epoch, num_epochs, i, len(dataloader),\n",
        "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "\n",
        "        # Save Losses for plotting later\n",
        "        G_losses.append(errG.item())\n",
        "        D_losses.append(errD.item())\n",
        "\n",
        "        # Check how the generator is doing by saving G's output on fixed_noise\n",
        "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
        "            with torch.no_grad():\n",
        "                fake = netG(fixed_noise).detach().cpu()\n",
        "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
        "\n",
        "        iters += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UqQ9WyU6cvv",
        "outputId": "3264906b-f24e-4638-c533-0222df70e2fd"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training Loop...\n",
            "[0/2][0/1875]\tLoss_D: 1.3600\tLoss_G: 20.9710\tD(x): 0.6210\tD(G(z)): 0.5646 / 0.0000\n",
            "[0/2][50/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][100/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][150/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][200/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][250/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][300/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][350/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][400/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][450/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][500/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][550/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][600/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][650/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][700/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][750/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][800/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][850/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][900/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][950/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][1000/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][1050/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][1100/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][1150/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][1200/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][1250/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][1300/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][1350/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][1400/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][1450/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][1500/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][1550/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][1600/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][1650/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][1700/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][1750/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][1800/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[0/2][1850/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][0/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][50/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][100/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][150/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][200/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][250/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][300/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][350/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][400/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][450/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][500/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][550/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][600/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][650/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][700/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][750/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][800/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][850/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][900/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][950/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][1000/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][1050/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][1100/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][1150/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][1200/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][1250/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][1300/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][1350/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][1400/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][1450/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][1500/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][1550/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][1600/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][1650/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][1700/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][1750/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][1800/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n",
            "[1/2][1850/1875]\tLoss_D: 100.0000\tLoss_G: 0.0000\tD(x): 1.0000\tD(G(z)): 1.0000 / 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XHEMiwOKBfgP"
      },
      "execution_count": 52,
      "outputs": []
    }
  ]
}